{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"%matplotlib inline"},{"cell_type":"markdown","metadata":{},"source":"\nWhat is PyTorch?\n================\n\nIt's a Python-based scientific computing package targeted at two sets of\naudiences:\n\n-  A replacement for NumPy to use the power of GPUs\n-  a deep learning research platform that provides maximum flexibility\n   and speed\n\nGetting Started\n---------------\n\nTensors\n^^^^^^^\n\nTensors are similar to NumPy's ndarrays, with the addition being that\nTensors can also be used on a GPU to accelerate computing.\n\n"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"from __future__ import print_function\nimport torch"},{"cell_type":"markdown","metadata":{},"source":["Construct a 5x3 matrix, uninitialized:\n\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[9.2755e-39, 1.0561e-38, 6.5204e-39],\n        [9.6429e-39, 1.0561e-38, 4.2246e-39],\n        [1.0286e-38, 1.0653e-38, 1.0194e-38],\n        [8.4490e-39, 1.0469e-38, 9.3674e-39],\n        [9.9184e-39, 8.7245e-39, 9.2755e-39]])\n"}],"source":"x = torch.empty(5, 3)\nprint(x)"},{"cell_type":"markdown","metadata":{},"source":["Construct a randomly initialized matrix:\n\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0.7982, 0.2758, 0.0669],\n        [0.8131, 0.8565, 0.4890],\n        [0.2035, 0.9608, 0.1109],\n        [0.1198, 0.5894, 0.3387],\n        [0.9609, 0.0064, 0.6470]])\n"}],"source":"x = torch.rand(5, 3)\nprint(x)"},{"cell_type":"markdown","metadata":{},"source":["Construct a matrix filled zeros and of dtype long:\n\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n"}],"source":"x = torch.zeros(5, 3, dtype=torch.long)\nprint(x)"},{"cell_type":"markdown","metadata":{},"source":["Construct a tensor directly from data:\n\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([5.5000, 3.0000])\n"}],"source":"x = torch.tensor([5.5, 3])\nprint(x)"},{"cell_type":"markdown","metadata":{},"source":["or create a tensor based on an existing tensor. These methods\nwill reuse properties of the input tensor, e.g. dtype, unless\nnew values are provided by user\n\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([[-0.4077,  0.7159,  2.3955],\n        [ 0.8951, -0.5194, -0.9790],\n        [-0.7959,  0.6044,  1.8815],\n        [ 1.6709,  1.1254,  1.2493],\n        [ 0.4440,  1.7768, -0.2146]])\n"}],"source":"x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\nprint(x)\n\nx = torch.randn_like(x, dtype=torch.float)    # override dtype!\nprint(x)                                      # result has the same size"},{"cell_type":"markdown","metadata":{},"source":["Get its size:\n\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([5, 3])\n"}],"source":"print(x.size())"},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div>\n\nOperations\n^^^^^^^^^^\nThere are multiple syntaxes for operations. In the following\nexample, we will take a look at the addition operation.\n\nAddition: syntax 1\n\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[ 0.0568,  1.5525,  3.0143],\n        [ 0.9785, -0.2423, -0.8597],\n        [ 0.1263,  0.9226,  1.9344],\n        [ 2.5806,  1.9981,  1.3145],\n        [ 0.8041,  2.4923,  0.2978]])\n"}],"source":"y = torch.rand(5, 3)\nprint(x + y)"},{"cell_type":"markdown","metadata":{},"source":["Addition: syntax 2\n\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[ 0.0568,  1.5525,  3.0143],\n        [ 0.9785, -0.2423, -0.8597],\n        [ 0.1263,  0.9226,  1.9344],\n        [ 2.5806,  1.9981,  1.3145],\n        [ 0.8041,  2.4923,  0.2978]])\n"}],"source":"print(torch.add(x, y))"},{"cell_type":"markdown","metadata":{},"source":["Addition: providing an output tensor as argument\n\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[ 0.0568,  1.5525,  3.0143],\n        [ 0.9785, -0.2423, -0.8597],\n        [ 0.1263,  0.9226,  1.9344],\n        [ 2.5806,  1.9981,  1.3145],\n        [ 0.8041,  2.4923,  0.2978]])\n"}],"source":"result = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)"},{"cell_type":"markdown","metadata":{},"source":["Addition: in-place\n\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[ 0.0568,  1.5525,  3.0143],\n        [ 0.9785, -0.2423, -0.8597],\n        [ 0.1263,  0.9226,  1.9344],\n        [ 2.5806,  1.9981,  1.3145],\n        [ 0.8041,  2.4923,  0.2978]])\n"}],"source":"# adds x to y\ny.add_(x)\nprint(y)"},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div>\n\nYou can use standard NumPy-like indexing with all bells and whistles!\n\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([ 0.7159, -0.5194,  0.6044,  1.1254,  1.7768])\n"}],"source":"print(x[:, 1])"},{"cell_type":"markdown","metadata":{},"source":["Resizing: If you want to resize/reshape tensor, you can use ``torch.view``:\n\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"}],"source":"x = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8)  # the size -1 is inferred from other dimensions\nprint(x.size(), y.size(), z.size())"},{"cell_type":"markdown","metadata":{},"source":["If you have a one element tensor, use ``.item()`` to get the value as a\nPython number\n\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([0.7188])\n0.7188162803649902\n"}],"source":"x = torch.randn(1)\nprint(x)\nprint(x.item())"},{"cell_type":"markdown","metadata":{},"source":["**Read later:**\n\n\n  100+ Tensor operations, including transposing, indexing, slicing,\n  mathematical operations, linear algebra, random numbers, etc.,\n  are described\n  `here <http://pytorch.org/docs/torch>`_.\n\nNumPy Bridge\n------------\n\nConverting a Torch Tensor to a NumPy array and vice versa is a breeze.\n\nThe Torch Tensor and NumPy array will share their underlying memory\nlocations, and changing one will change the other.\n\nConverting a Torch Tensor to a NumPy Array\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([1., 1., 1., 1., 1.])\n"}],"source":"a = torch.ones(5)\nprint(a)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[1. 1. 1. 1. 1.]\n"}],"source":"b = a.numpy()\nprint(b)"},{"cell_type":"markdown","metadata":{},"source":["See how the numpy array changed in value.\n\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([2., 2., 2., 2., 2.])\n[2. 2. 2. 2. 2.]\n"}],"source":"a.add_(1)\nprint(a)\nprint(b)"},{"cell_type":"markdown","metadata":{},"source":["Converting NumPy Array to Torch Tensor\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nSee how changing the np array changed the Torch Tensor automatically\n\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[2. 2. 2. 2. 2.]\ntensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"}],"source":"import numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)"},{"cell_type":"markdown","metadata":{},"source":["All the Tensors on the CPU except a CharTensor support converting to\nNumPy and back.\n\nCUDA Tensors\n------------\n\nTensors can be moved onto any device using the ``.to`` method.\n\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":"False"},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":"# let us run this cell only if CUDA is available\n# We will use ``torch.device`` objects to move tensors in and out of GPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n    z = x + y\n    print(z)\n    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\ntorch.cuda.is_available()"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}